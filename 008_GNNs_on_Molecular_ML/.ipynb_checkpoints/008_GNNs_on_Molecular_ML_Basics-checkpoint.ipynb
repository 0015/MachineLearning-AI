{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 0. ... 0. 0. 0.]\n",
      "(2048,)\n",
      "54.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.AllChem import GetMorganFingerprintAsBitVect\n",
    "from rdkit.DataStructs.cDataStructs import ConvertToNumpyArray\n",
    "import numpy as np\n",
    "import deepchem as dc\n",
    "import torch_geometric\n",
    "\n",
    "atorvastatin_smiles = 'O=C(O)C[C@H](O)C[C@H](O)CCn2c(c(c(c2c1ccc(F)cc1)c3ccccc3)C(=O)Nc4ccccc4)'\n",
    "atorvastatin = Chem.MolFromSmiles(atorvastatin_smiles)\n",
    "\n",
    "fing_print = GetMorganFingerprintAsBitVect(atorvastatin, radius = 2, nBits = 2048)\n",
    "\n",
    "fp_array = np.zeros((1, ))\n",
    "\n",
    "ConvertToNumpyArray(fing_print, fp_array)\n",
    "print(fp_array)\n",
    "print(fp_array.shape)\n",
    "print(sum(fp_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_atom_features(mol):\n",
    "    ato_number = []\n",
    "    num_h = []\n",
    "    \n",
    "    for atom in mol.GetAtoms(): \n",
    "        ato_number.append(atom.GetAtomicNum())\n",
    "        num_h.append(atom.GetTotalNumHs(includeNeighbors = True))\n",
    "        \n",
    "    return torch.tensor([ato_number, num_h]).t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edge_index(mol): \n",
    "    r, c = [], []\n",
    "    \n",
    "    for bond in mol.GetBonds():\n",
    "        st, end = bond.GetBeginAtomIdx(),  bond.GetEndAtomIdx()\n",
    "        r += [st, end]\n",
    "        c += [end, st]\n",
    "        \n",
    "    return torch.tensor([r, c], dtype = torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "def prepare_dataloader(mol_list, batch_size=3):\n",
    "    data_list = []\n",
    "\n",
    "    for i, mol in enumerate(mol_list):\n",
    "        \n",
    "        x = get_atom_features(mol)\n",
    "        edge_index = get_edge_index(mol)\n",
    "\n",
    "        data = torch_geometric.data.data.Data(edge_index=edge_index, x=x)\n",
    "        data_list.append(data)\n",
    "\n",
    "    return DataLoader(data_list, batch_size=batch_size, shuffle=False), data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Data(x=[22, 2], edge_index=[2, 46]), Data(x=[32, 2], edge_index=[2, 66]), Data(x=[18, 2], edge_index=[2, 38]), Data(x=[20, 2], edge_index=[2, 42]), Data(x=[19, 2], edge_index=[2, 40]), Data(x=[31, 2], edge_index=[2, 68])]\n",
      "<torch_geometric.loader.dataloader.DataLoader object at 0x000001A8F9302208>\n"
     ]
    }
   ],
   "source": [
    "smiles_list = ['Cc1cc(c(C)n1c2ccc(F)cc2)S(=O)(=O)NCC(=O)N',\n",
    "'CN(CC(=O)N)S(=O)(=O)c1c(C)n(c(C)c1S(=O)(=O)N(C)CC(=O)N)c2ccc(F)cc2',\n",
    "'Fc1ccc(cc1)n2cc(COC(=O)CBr)nn2',\n",
    "'CCOC(=O)COCc1cn(nn1)c2ccc(F)cc2',\n",
    "'COC(=O)COCc1cn(nn1)c2ccc(F)cc2',\n",
    "'Fc1ccc(cc1)n2cc(COCC(=O)OCc3cn(nn3)c4ccc(F)cc4)nn2']\n",
    "\n",
    "mol_list = [Chem.MolFromSmiles(smi) for smi in smiles_list]\n",
    "dloader, dList = prepare_dataloader(mol_list)\n",
    "print(dList)\n",
    "print(dloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[72, 2], edge_index=[2, 150], batch=[72], ptr=[4])\n"
     ]
    }
   ],
   "source": [
    "for batch in dloader:\n",
    "    break\n",
    "    \n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_scatter import scatter_add\n",
    "from torch_geometric.utils import add_self_loops, degree\n",
    "\n",
    "class NeuralLoop(MessagePassing):\n",
    "    def __init__(self, atom_features, fp_size):\n",
    "        super(NeuralLoop, self).__init__(aggr='add')\n",
    "        self.H = nn.Linear(atom_features, atom_features)\n",
    "        self.W = nn.Linear(atom_features, fp_size)\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
    "        return self.propagate(edge_index, size=(x.size(0), x.size(0)), x=x)\n",
    "    \n",
    "    def message(self, x_j, edge_index, size):\n",
    "        return x_j \n",
    "    \n",
    "    def update(self, v):\n",
    "        v = v.type('torch.FloatTensor')\n",
    "        updated_atom_features = self.H(v).sigmoid()\n",
    "        updated_fingerprint = self.W(updated_atom_features).softmax(dim=-1)\n",
    "        \n",
    "        return updated_atom_features, updated_fingerprint # shape [N, atom_features]\n",
    "    \n",
    "class NeuralFP(nn.Module):\n",
    "    def __init__(self, atom_features=52, fp_size=2048):\n",
    "        super(NeuralFP, self).__init__()\n",
    "        \n",
    "        self.atom_features = 52\n",
    "        self.fp_size = 2048\n",
    "        \n",
    "        self.loop1 = NeuralLoop(atom_features=atom_features, fp_size=fp_size)\n",
    "        self.loop2 = NeuralLoop(atom_features=atom_features, fp_size=fp_size)\n",
    "        self.loops = nn.ModuleList([self.loop1, self.loop2])\n",
    "        \n",
    "    def forward(self, data):\n",
    "        fingerprint = torch.zeros((data.batch.shape[0], self.fp_size), dtype=torch.float)\n",
    "        \n",
    "        out = data.x\n",
    "        for idx, loop in enumerate(self.loops):\n",
    "            updated_atom_features, updated_fingerprint = loop(out, data.edge_index)\n",
    "            out = updated_atom_features\n",
    "            fingerprint += updated_fingerprint\n",
    "            \n",
    "        return scatter_add(fingerprint, data.batch, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2048])\n"
     ]
    }
   ],
   "source": [
    "neural_fp = NeuralFP(atom_features = 2, fp_size = 2048)\n",
    "fps = neural_fp(batch)\n",
    "print(fps.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<rdkit.Chem.rdchem.Mol object at 0x000001A8F92F72D0>\n",
      " <rdkit.Chem.rdchem.Mol object at 0x000001A8F92F7298>\n",
      " <rdkit.Chem.rdchem.Mol object at 0x000001A8F92F7260> ...\n",
      " <rdkit.Chem.rdchem.Mol object at 0x000001A8F92AA378>\n",
      " <rdkit.Chem.rdchem.Mol object at 0x000001A8F92AA458>\n",
      " <rdkit.Chem.rdchem.Mol object at 0x000001A8F92AA538>]\n"
     ]
    }
   ],
   "source": [
    "_, (train, valid, test), _ = dc.molnet.load_bace_regression(featurizer = 'Raw')\n",
    "bs = 4\n",
    "print(train.X)\n",
    "train_loader, _ = prepare_dataloader(train.X, batch_size = bs)\n",
    "valid_loader, _ = prepare_dataloader(valid.X, batch_size = bs)\n",
    "test_loader, _ = prepare_dataloader(test.X, batch_size = bs)\n",
    "\n",
    "train_labels_loader = torch.utils.data.DataLoader(train.y, batch_size = bs)\n",
    "valid_labels_loader = torch.utils.data.DataLoader(valid.y, batch_size = bs)\n",
    "test_labels_loader = torch.utils.data.DataLoader(test.y, batch_size = bs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n"
     ]
    }
   ],
   "source": [
    "print(len(valid_labels_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class MLP_regressor(nn.Module): \n",
    "    def __init__(self, atom_features = 2, fp_size = 2048, hidden_size = 100): \n",
    "        super(MLP_regressor, self).__init__()\n",
    "        self.neural_fp = neural_fp\n",
    "        self.lin1 = nn.Linear(fp_size, hidden_size)\n",
    "        self.lin2 = nn.Linear(hidden_size, 1)\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        \n",
    "    def forward(self, batch): \n",
    "        fp = self.neural_fp(batch)\n",
    "        hidden = F.relu(self.dropout(self.lin1(fp)))\n",
    "        out = F.relu(self.lin2(hidden))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(batch, labels, reg): \n",
    "    out = reg(batch)\n",
    "    loss = F.mse_loss(out, labels.to(torch.float), reduction = 'mean')\n",
    "    loss.backward()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_step(batch, labels, reg): \n",
    "    out = reg(batch)\n",
    "    loss = F.mse_loss(out, labels.to(torch.float), reduction = 'mean')\n",
    "    # loss.backward()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(train_loader, train_labels_loader, reg, opt): \n",
    "    reg.train()\n",
    "    total_loss = 0\n",
    "    for idx, (batch, labels) in enumerate(zip(train_loader, train_labels_loader)):\n",
    "        loss = train_step(batch, labels, reg)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    torch.nn.utils.clip_grad_norm_(reg.parameters(), 1)\n",
    "    opt.step()\n",
    "    opt.zero_grad()\n",
    "    return total_loss/len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_fn(valid_loader, valid_labels_loader, reg): \n",
    "    reg.train()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for idx, (batch, labels) in enumerate(zip(valid_loader, valid_labels_loader)):\n",
    "            loss = valid_step(batch, labels, reg)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    total_loss /= len(valid_loader)\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10   train_loss: 1.0028532254709417   valid_loss: 0.314499016242304\n",
      "Epoch:20   train_loss: 1.000763944407046   valid_loss: 0.32039252947135155\n",
      "Epoch:30   train_loss: 0.9989624869765363   valid_loss: 0.32534536729049013\n",
      "Epoch:40   train_loss: 0.9987429508688881   valid_loss: 0.3307444569233523\n",
      "Epoch:50   train_loss: 0.9973115894940402   valid_loss: 0.33397750943664795\n",
      "Epoch:60   train_loss: 0.9958178968377804   valid_loss: 0.3380616707325221\n",
      "Epoch:70   train_loss: 0.9951895318876202   valid_loss: 0.34095786681064055\n",
      "Epoch:80   train_loss: 0.9938792334362349   valid_loss: 0.34409069778676465\n",
      "Epoch:90   train_loss: 0.9929891601320134   valid_loss: 0.3444661681181682\n",
      "Epoch:100   train_loss: 0.9915923232208053   valid_loss: 0.3461741656122429\n",
      "Epoch:110   train_loss: 0.99160590018845   valid_loss: 0.3450323944497014\n",
      "Epoch:120   train_loss: 0.9915192742069734   valid_loss: 0.3455415155999378\n",
      "Epoch:130   train_loss: 0.9920001698180683   valid_loss: 0.3446881283579339\n",
      "Epoch:140   train_loss: 0.9911336467682811   valid_loss: 0.3444673014447816\n",
      "Epoch:150   train_loss: 0.9907798093625286   valid_loss: 0.3465002076907599\n",
      "Epoch:160   train_loss: 0.9920237887175846   valid_loss: 0.3450635200302043\n",
      "Epoch:170   train_loss: 0.992192179920911   valid_loss: 0.3458843055257028\n",
      "Epoch:180   train_loss: 0.9910299465615032   valid_loss: 0.34756949914222596\n",
      "Epoch:190   train_loss: 0.9904199603702197   valid_loss: 0.34524030741809647\n",
      "Epoch:200   train_loss: 0.9907604413139312   valid_loss: 0.3462698506763876\n",
      "Epoch:210   train_loss: 0.990725856591723   valid_loss: 0.34559948375049554\n",
      "Epoch:220   train_loss: 0.9908209963126888   valid_loss: 0.3459809221711016\n",
      "Epoch:230   train_loss: 0.9906890147600765   valid_loss: 0.3452728820894232\n",
      "Epoch:240   train_loss: 0.9904954457922959   valid_loss: 0.3457035123352206\n",
      "Epoch:250   train_loss: 0.9906301379753983   valid_loss: 0.3466181445856867\n",
      "Epoch:260   train_loss: 0.9905039000873556   valid_loss: 0.34434362228708587\n",
      "Epoch:270   train_loss: 0.9910890035647838   valid_loss: 0.3472403859823795\n",
      "Epoch:280   train_loss: 0.9905172102890982   valid_loss: 0.3455717474334003\n",
      "Epoch:290   train_loss: 0.9907852006290822   valid_loss: 0.345730846977906\n",
      "Epoch:300   train_loss: 0.9908489774041979   valid_loss: 0.3461254008198969\n",
      "Epoch:310   train_loss: 0.9908097195123989   valid_loss: 0.3448252456848997\n",
      "Epoch:320   train_loss: 0.9899742773829316   valid_loss: 0.3456139215321807\n",
      "Epoch:330   train_loss: 0.9910709037009025   valid_loss: 0.34377438330078514\n",
      "Epoch:340   train_loss: 0.9909052582466883   valid_loss: 0.3458074407484865\n",
      "Epoch:350   train_loss: 0.9908010351238004   valid_loss: 0.34592125719274536\n",
      "Epoch:360   train_loss: 0.9909392712605508   valid_loss: 0.345464220609038\n",
      "Epoch:370   train_loss: 0.9908911940582533   valid_loss: 0.34563207597621887\n",
      "Epoch:380   train_loss: 0.9908936727744383   valid_loss: 0.3447263337336488\n",
      "Epoch:390   train_loss: 0.9901856894085411   valid_loss: 0.3460142945054701\n",
      "Epoch:400   train_loss: 0.9910872772554237   valid_loss: 0.34549614359313735\n",
      "Epoch:410   train_loss: 0.9899862595163778   valid_loss: 0.34335810176708037\n",
      "Epoch:420   train_loss: 0.9907061820242726   valid_loss: 0.3450648055213127\n",
      "Epoch:430   train_loss: 0.9910852129689476   valid_loss: 0.3459243847853366\n",
      "Epoch:440   train_loss: 0.9908327561362859   valid_loss: 0.34516414791409933\n",
      "Epoch:450   train_loss: 0.991354488007334   valid_loss: 0.3459859692658555\n",
      "Epoch:460   train_loss: 0.9908954846079495   valid_loss: 0.3459026565638181\n",
      "Epoch:470   train_loss: 0.9902421823163323   valid_loss: 0.34531102302226874\n",
      "Epoch:480   train_loss: 0.9907394984541861   valid_loss: 0.34510484216030146\n",
      "Epoch:490   train_loss: 0.9908999465160048   valid_loss: 0.346456584614651\n",
      "Epoch:500   train_loss: 0.9906420510134694   valid_loss: 0.34526169860423817\n",
      "Epoch:510   train_loss: 0.9902654173614907   valid_loss: 0.34593828840948654\n",
      "Epoch:520   train_loss: 0.9903211944564415   valid_loss: 0.3445308037338883\n",
      "Epoch:530   train_loss: 0.9913069229402423   valid_loss: 0.3461812434214057\n",
      "Epoch:540   train_loss: 0.9903639474232057   valid_loss: 0.3444557149073501\n",
      "Epoch:550   train_loss: 0.9910361678831329   valid_loss: 0.34578587654840176\n",
      "Epoch:560   train_loss: 0.990507168330332   valid_loss: 0.3458084692233809\n",
      "Epoch:570   train_loss: 0.9908139103126063   valid_loss: 0.3457162304992763\n",
      "Epoch:580   train_loss: 0.9909038454841318   valid_loss: 0.34588498864200357\n",
      "Epoch:590   train_loss: 0.9905045270856789   valid_loss: 0.3457416253515571\n",
      "Epoch:600   train_loss: 0.9915542160959995   valid_loss: 0.3449237926717232\n",
      "Epoch:610   train_loss: 0.9906960974540407   valid_loss: 0.3454572114454717\n",
      "Epoch:620   train_loss: 0.9900674597715972   valid_loss: 0.34538286635008836\n",
      "Epoch:630   train_loss: 0.9901427513506362   valid_loss: 0.3450932710905985\n",
      "Epoch:640   train_loss: 0.9900860891649776   valid_loss: 0.345803882302119\n",
      "Epoch:650   train_loss: 0.990588072250926   valid_loss: 0.34617517609264103\n",
      "Epoch:660   train_loss: 0.9907233570064587   valid_loss: 0.34456380484238924\n",
      "Epoch:670   train_loss: 0.9908544609327754   valid_loss: 0.34530054010362\n",
      "Epoch:680   train_loss: 0.9904447426764362   valid_loss: 0.34543808536986226\n",
      "Epoch:690   train_loss: 0.9902962374089058   valid_loss: 0.3444611312787808\n",
      "Epoch:700   train_loss: 0.9905680148591819   valid_loss: 0.34417051437317003\n",
      "Epoch:710   train_loss: 0.9913424400554313   valid_loss: 0.3440265379461045\n",
      "Epoch:720   train_loss: 0.9911784781488857   valid_loss: 0.3440833752792969\n",
      "Epoch:730   train_loss: 0.9906703680169711   valid_loss: 0.34568298984346546\n",
      "Epoch:740   train_loss: 0.9913926348493591   valid_loss: 0.3455501029828539\n",
      "Epoch:750   train_loss: 0.9905840347785343   valid_loss: 0.34534805740450364\n",
      "Epoch:760   train_loss: 0.9901549732089718   valid_loss: 0.3470394457131318\n",
      "Epoch:770   train_loss: 0.9909562595371502   valid_loss: 0.34628273527548453\n",
      "Epoch:780   train_loss: 0.9907633347041819   valid_loss: 0.346569528191283\n",
      "Epoch:790   train_loss: 0.9910424461399233   valid_loss: 0.34487668602397253\n",
      "Epoch:800   train_loss: 0.9908300869354345   valid_loss: 0.3459403576598561\n",
      "Epoch:810   train_loss: 0.9906613091725346   valid_loss: 0.3455861760414432\n",
      "Epoch:820   train_loss: 0.9898971657048935   valid_loss: 0.344987575716866\n",
      "Epoch:830   train_loss: 0.9908534560822068   valid_loss: 0.34561482289657425\n",
      "Epoch:840   train_loss: 0.9907407250548114   valid_loss: 0.34632187173050816\n",
      "Epoch:850   train_loss: 0.9910976447698294   valid_loss: 0.3456738688109908\n",
      "Epoch:860   train_loss: 0.9903611498831068   valid_loss: 0.3454308066757221\n",
      "Epoch:870   train_loss: 0.9908916299064604   valid_loss: 0.34654174117435776\n",
      "Epoch:880   train_loss: 0.9906236266543652   valid_loss: 0.3459222463318024\n",
      "Epoch:890   train_loss: 0.9908384182959143   valid_loss: 0.346011802903568\n",
      "Epoch:900   train_loss: 0.9909366911827362   valid_loss: 0.3448604948575131\n",
      "Epoch:910   train_loss: 0.9903766129407171   valid_loss: 0.3458515206856598\n",
      "Epoch:920   train_loss: 0.9905556889801062   valid_loss: 0.34621894438604567\n",
      "Epoch:930   train_loss: 0.9916055700188448   valid_loss: 0.3474877821653009\n",
      "Epoch:940   train_loss: 0.9904839075931191   valid_loss: 0.3447760870905385\n",
      "Epoch:950   train_loss: 0.9915171574452345   valid_loss: 0.3458469020712839\n",
      "Epoch:960   train_loss: 0.9908000960425095   valid_loss: 0.3449226082402196\n",
      "Epoch:970   train_loss: 0.990870048692327   valid_loss: 0.34600445485251075\n",
      "Epoch:980   train_loss: 0.9903246838462665   valid_loss: 0.34594876819449527\n",
      "Epoch:990   train_loss: 0.9914150753707038   valid_loss: 0.34579894909467457\n",
      "Epoch:1000   train_loss: 0.990959428847082   valid_loss: 0.34454074059708784\n"
     ]
    }
   ],
   "source": [
    "reg = MLP_regressor(atom_features = 2, fp_size = 2048, hidden_size = 100)\n",
    "optimizer = torch.optim.SGD(reg.parameters(), lr = 0.001, weight_decay = 0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience = 100)\n",
    "\n",
    "total_epochs = 1000\n",
    "\n",
    "history = dict()\n",
    "tr_l, va_l = list(), list()\n",
    "for epoch in range(1, total_epochs+1): \n",
    "    train_loss = train_fn(train_loader, train_labels_loader, reg, opt= optimizer)\n",
    "    valid_loss = valid_fn(valid_loader, valid_labels_loader, reg) \n",
    "    \n",
    "    scheduler.step(valid_loss)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch:{epoch}   train_loss: {train_loss}   valid_loss: {valid_loss}')\n",
    "        \n",
    "    tr_l.append(train_loss)\n",
    "    va_l.append(valid_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "style.use('fivethirtyeight')\n",
    "sns.set(style='whitegrid',color_codes=True)\n",
    "\n",
    "# summarize history for loss\n",
    "plt.figure(figsize = (20,4))\n",
    "plt.plot(tr_l)\n",
    "plt.plot(va_l)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.savefig('./loss_1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "molecularML",
   "language": "python",
   "name": "molecularml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
