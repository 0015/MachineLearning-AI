{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acer\\Anaconda3\\envs\\embvr\\lib\\site-packages\\h5py\\__init__.py:72: UserWarning: h5py is running against HDF5 1.10.2 when it was built against 1.10.3, this may cause problems\n",
      "  '{0}.{1}.{2}'.format(*version.hdf5_built_version_tuple)\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import pathlib\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35553\n",
      "8888\n"
     ]
    }
   ],
   "source": [
    "## create train and validation datasets\n",
    "DB_PATH = \"./data/images\"\n",
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 1000\n",
    "IMG_WIDTH = 60\n",
    "IMG_HEIGHT = 60\n",
    "\n",
    "def load(image_file):\n",
    "    image = tf.io.read_file(image_file)\n",
    "    image = tf.image.decode_jpeg(image,channels=3)\n",
    "\n",
    "    input_image = tf.cast(image, tf.float32)\n",
    "    return input_image\n",
    "\n",
    "def random_crop(input_image):\n",
    "    cropped_image = tf.image.random_crop(\n",
    "      input_image, size=[IMG_HEIGHT, IMG_WIDTH, 3])\n",
    "\n",
    "    return cropped_image\n",
    "\n",
    "def resize(input_image):\n",
    "    input_image = tf.image.resize(input_image, [IMG_HEIGHT, IMG_WIDTH],\n",
    "                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    return input_image\n",
    "\n",
    "def normalize(input_image):\n",
    "    input_image = (input_image / 255)\n",
    "    return input_image\n",
    "\n",
    "@tf.function()\n",
    "def random_jitter(input_image):\n",
    "    input_image = random_crop(input_image)\n",
    "\n",
    "    if tf.random.uniform(()) > 0.5:\n",
    "        input_image = tf.image.flip_left_right(input_image)\n",
    "        \n",
    "    return input_image\n",
    "\n",
    "def load_image_train(image_file):\n",
    "    input_image = load(image_file)\n",
    "    #input_image = random_jitter(input_image)\n",
    "    input_image = resize(input_image)\n",
    "    input_image = normalize(input_image)\n",
    "\n",
    "    return input_image,input_image\n",
    "\n",
    "def load_image_test(image_file):\n",
    "    input_image = load(image_file)\n",
    "    #input_image = random_jitter(input_image)\n",
    "    input_image = resize(input_image)\n",
    "    input_image = normalize(input_image)\n",
    "\n",
    "    return input_image,input_image\n",
    "\n",
    "\n",
    "data_dir = pathlib.Path(DB_PATH)\n",
    "image_count = len(list(data_dir.glob('*.jpg')))\n",
    "dataset = tf.data.Dataset.list_files(DB_PATH+'\\\\*.jpg')\n",
    "\n",
    "val_size = int(image_count * 0.2)\n",
    "train_ds = dataset.skip(val_size)\n",
    "val_ds = dataset.take(val_size)\n",
    "\n",
    "print(tf.data.experimental.cardinality(train_ds).numpy())\n",
    "print(tf.data.experimental.cardinality(val_ds).numpy())\n",
    "\n",
    "\n",
    "train_ds = train_ds.map(load_image_train, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "train_ds = train_ds.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "val_ds = val_ds.map(load_image_test)\n",
    "val_ds = val_ds.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TakeDataset shapes: ((None, 60, 60, 3), (None, 60, 60, 3)), types: (tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_ds.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "for images,targets in val_ds.take(1):\n",
    "    for i in range(45):\n",
    "        ax = plt.subplot(3, 15, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"float32\"))\n",
    "        plt.axis(\"off\")\n",
    "plt.savefig(\"./Images/VisualizingSampleImages.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Mat Shape:\n",
      "(1000, 3600)\n",
      "No. of PCA Features:\n",
      "(1000, 32)\n"
     ]
    }
   ],
   "source": [
    "# Embedding Images to PCA Space\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "pca = PCA(32)\n",
    "imgs_list = []\n",
    "vis_imgs = []\n",
    "for images,targets in val_ds.take(1):\n",
    "    for i in range(BATCH_SIZE):\n",
    "        vis_imgs.append(tf.keras.preprocessing.image.img_to_array(images[i]))\n",
    "        image = tf.image.rgb_to_grayscale(images[i])\n",
    "        img_arr = tf.keras.preprocessing.image.img_to_array(image)\n",
    "        imgs_list.append(img_arr.ravel())\n",
    "img_mat = np.array(imgs_list)\n",
    "print(\"Image Mat Shape:\")\n",
    "print(img_mat.shape)\n",
    "pca_feat = pca.fit_transform(img_mat)\n",
    "print(\"No. of PCA Features:\")\n",
    "print(pca_feat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.4384224 ,  6.3139334 , -5.5071607 , ..., -0.21874347,\n",
       "         0.8788698 ,  0.44343644],\n",
       "       [ 4.933215  , -2.048595  , 10.363574  , ...,  0.7182399 ,\n",
       "         1.4535775 , -0.6461782 ],\n",
       "       [ 8.618038  , -3.412127  ,  2.8519685 , ...,  3.9372644 ,\n",
       "        -1.5679541 , -1.569178  ],\n",
       "       ...,\n",
       "       [ 5.819934  , 15.910091  , -4.7893    , ...,  1.0169804 ,\n",
       "        -1.6699593 , -0.56989115],\n",
       "       [12.058035  , -2.6452725 , -2.7738216 , ...,  0.25707898,\n",
       "         1.4338689 ,  0.71756935],\n",
       "       [ 4.8713512 , -5.6426983 , 11.02865   , ...,  0.2113827 ,\n",
       "         1.7766013 , -1.1848415 ]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def visualize_space(X,images,outfile):\n",
    "\n",
    "    tsne = TSNE(n_components=2, init='random').fit_transform(X)\n",
    "    tx, ty = tsne[:,0], tsne[:,1]\n",
    "    tx = (tx-np.min(tx)) / (np.max(tx) - np.min(tx))\n",
    "    ty = (ty-np.min(ty)) / (np.max(ty) - np.min(ty))\n",
    "    width = 4000\n",
    "    height = 3000\n",
    "\n",
    "    full_image = Image.new('RGBA', (width, height))\n",
    "    for img, x, y in zip(images, tx, ty):\n",
    "        img = np.array(img)\n",
    "        tile = Image.fromarray(np.uint8(img*255))\n",
    "        full_image.paste(tile, (int((width)*x), int((height)*y)), mask=tile.convert('RGBA'))\n",
    "\n",
    "    plt.figure(figsize = (66,50))\n",
    "    plt.imshow(full_image)\n",
    "    plt.axis(\"off\")\n",
    "    full_image.save(outfile)\n",
    "\n",
    "X = np.array(pca_feat)\n",
    "visualize_space(X,vis_imgs,\"./Images/tSNE-PCA-fashiondb.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 8, 8, 1)\n",
      "(None, 32)\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 60, 60, 3)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 60, 60, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 30, 30, 16)        4624      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 15, 15, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 15, 15, 4)         580       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 4)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 8, 8, 1)           37        \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                2080      \n",
      "=================================================================\n",
      "Total params: 12,377\n",
      "Trainable params: 12,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "batch_size = 16\n",
    "latent_dim = 32  # Number of latent dimension parameters\n",
    "\n",
    "input_img = Input(shape=(60, 60, 3))\n",
    "\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)\n",
    "x = MaxPool2D( (2, 2), padding='same')(x)\n",
    "\n",
    "x = Conv2D(16, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPool2D( (2, 2), padding='same')(x)\n",
    "\n",
    "x = Conv2D(4, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPool2D( (2, 2), padding='same')(x)\n",
    "\n",
    "x = Conv2D(1, (3, 3), activation='relu', padding='same')(x)\n",
    "\n",
    "shape_before_flattening = K.int_shape(x)\n",
    "print(shape_before_flattening)\n",
    "x = Flatten()(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "\n",
    "Z = Dense(latent_dim)(x)\n",
    "print(K.int_shape(Z))\n",
    "\n",
    "encoder = Model(input_img,Z)\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 32)]              0         \n",
      "_________________________________________________________________\n",
      "intermediate_decoder (Dense) (None, 900)               29700     \n",
      "_________________________________________________________________\n",
      "original_decoder (Dense)     (None, 900)               810900    \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 15, 15, 4)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 15, 15, 3)         111       \n",
      "_________________________________________________________________\n",
      "up_sampling2d (UpSampling2D) (None, 30, 30, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 30, 30, 3)         84        \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 60, 60, 3)         0         \n",
      "=================================================================\n",
      "Total params: 840,795\n",
      "Trainable params: 840,795\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# decoder takes the latent distribution sample as input\n",
    "decoder_input = Input(K.int_shape(Z)[1:])\n",
    "x = Dense(15*15*4, activation='relu', name=\"intermediate_decoder\", input_shape=(latent_dim,))(decoder_input)\n",
    "# Expand to 900 total pixels\n",
    "x = Dense(900, activation='sigmoid', name=\"original_decoder\")(x)\n",
    "x = Reshape((15,15,4),input_shape=(900,))(x)\n",
    "\n",
    "x = Conv2DTranspose(3, (3, 3), padding='same')(x)\n",
    "x = UpSampling2D( (2, 2))(x)\n",
    "\n",
    "x = Conv2DTranspose(3, (3, 3), padding='same')(x)\n",
    "x = UpSampling2D( (2, 2))(x)\n",
    "\n",
    "\n",
    "# decoder model statement\n",
    "decoder = Model(decoder_input, x)\n",
    "\n",
    "# apply the decoder to the sample from the latent distribution\n",
    "z_decoded = decoder(Z)\n",
    "\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 60, 60, 3)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 60, 60, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 30, 30, 16)        4624      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 15, 15, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 15, 15, 4)         580       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 4)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 8, 8, 1)           37        \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "model_1 (Model)              (None, 60, 60, 3)         840795    \n",
      "=================================================================\n",
      "Total params: 853,172\n",
      "Trainable params: 853,172\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# VAE model statement\n",
    "ae = Model(input_img,z_decoded)\n",
    "ae.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "ae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 36 steps, validate for 9 steps\n",
      "Epoch 1/10\n",
      " 3/36 [=>............................] - ETA: 24:28 - loss: 0.0269"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "# run the model\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=5, mode='auto')\n",
    "history = ae.fit(train_ds, epochs = 10, validation_data=val_ds, callbacks=[early_stopping],verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of Latent Space:\n",
      "(1000, 32)\n"
     ]
    }
   ],
   "source": [
    "# Project validation set into Latent Space\n",
    "vis_imgs = []\n",
    "for input_images,output_images in val_ds.take(1):\n",
    "    latent_vec = encoder(input_images)\n",
    "    for i in range(BATCH_SIZE):\n",
    "        vis_imgs.append(tf.keras.preprocessing.image.img_to_array(input_images[i]))\n",
    "print(\"Dimension of Latent Space:\")\n",
    "print(latent_vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= np.array(latent_vec)\n",
    "\n",
    "visualize_space(X,vis_imgs,\"./Images/tSNE-Latent-fashiondb.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "computerVision",
   "language": "python",
   "name": "computervision"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
